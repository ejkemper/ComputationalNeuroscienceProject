#Import preprocessed data
#Import packages
import tensorflow as tf
from tensorflow import keras
from scipy.io import loadmat, savemat
from os import listdir
from os.path import dirname, join as join
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

print(tf.__version__)


#import visual representation - mnist
mnist =  keras.datasets.mnist
(train_imgs_mnist, train_labs_mnist), (test_imgs_mnist, test_labs_mnist) = mnist.load_data()

train_imgs_mnist = train_imgs_mnist/255.0
test_imgs_mnist = test_imgs_mnist/255.0

#import cochleagrams 
def load_files(dir): #coch=15,53,
    filenames = listdir(dir)
    files = np.zeros((len(filenames), 15, 53))
    labels = []
    for i in range(len(filenames)):
        file = np.load(join(dir, filenames[i]))
        files[i] = file
        label = int(filenames[i][0]) #to get the 6th character in the string, which gives the digit pronounced
        labels.append(label)
    labels = np.array(labels)
    return files, labels

dir = "C:/Users/s141554/Documents/Systems Biology/Computational neuroscience/cochleagrams-preprocessed"
data_coch, labels_coch = load_files(dir)
train_data_ch, test_data_ch, train_label_ch, test_label_ch = train_test_split(data_coch, labels_coch, test_size=0.2, random_state=42)



#Input - make sure for every number the amount of visual and auditory presentations are the same 
def match_data(data_mnist, labels_mnist, data_coch, labels_coch):
    data_ch = []
    data_m = []
    labels = []
    for x in range(10):
        nr_mnist = len(labels_mnist[labels_mnist == x])
        nr_coch = len(labels_coch[labels_coch == x])
        reps = np.ceil(nr_mnist/nr_coch)

        rep_data_ch = np.repeat(data_coch[labels_coch == x], reps, axis=0)
        rep_data_ch = rep_data_ch[:nr_mnist]
        data_ch.append(rep_data_ch)

        data_m.append(data_mnist[labels_mnist == x])
        labels.append(labels_mnist[labels_mnist == x])

    data_ch = np.concatenate(data_ch, axis=0)
    data_m = np.concatenate(data_m, axis=0)
    labels = np.concatenate(labels, axis=0)
    return data_ch, data_m, labels
#output (created with match_data)
train_data_cochleagrams, train_data_visual, train_label = match_data(train_imgs_mnist, train_labs_mnist, train_data_ch, train_label_ch)
test_data_cochleagrams, test_data_visual, test_label = match_data(test_imgs_mnist, test_labs_mnist, test_data_ch, test_label_ch)

#create random index order

index_visual=np.arange(60000)
np.random.shuffle(index_visual)

index_auditory=np.arange(60000)
np.random.shuffle(index_auditory)


index_visual_test = np.arange(10000)
np.random.shuffle(index_visual_test)
index_auditory_test = np.arange(10000)
np.random.shuffle(index_auditory_test)
#randomize the order of the cochleagrams and mnist while keeping them coupled to the right label

train_cochleagrams = train_data_cochleagrams[index_auditory]
train_mnist= train_data_visual[index_visual]
train_labels_cochleagrams = train_label[index_auditory]
train_labels_mnist = train_label[index_visual]


test_cochleagrams = test_data_cochleagrams[index_auditory_test]
test_mnist = test_data_visual[index_visual_test]
test_labels_cochleagrams = test_label[index_auditory_test]
test_labels_mnist = test_label[index_visual_test]

### Prepare data for the task classifiers 
#Break training data into blocks
vis1 = train_mnist[:30000]
vis2 = train_mnist[30000:]
vis1_lab = train_labels_mnist[:30000] 
vis2_lab = train_labels_mnist[30000:]

aud1 = train_cochleagrams[:30000]
aud2 = train_cochleagrams[30000:]
aud1_lab = train_labels_cochleagrams[:30000]
aud2_lab = train_labels_cochleagrams[30000:]

zeros = np.zeros((30000,), dtype=int)
empty_mnist = np.zeros((28,28),dtype=int)
zeros_mnist = np.repeat([empty_mnist], 30000, axis=0)
empty_coch = np.zeros((15,53),dtype=int)
zeros_coch = np.repeat([empty_coch], 30000, axis =0)

#create 4 input streams
## vis1  vis2  0    0 
## vis2   0    0    aud1
## 0     vis1 aud2  0 
## 0      0   aud1  aud2
in_vis_1 = np.concatenate((vis1, vis2, zeros_mnist, zeros_mnist))
labels_in_vis_1 = np.concatenate((vis1_lab, vis2_lab, zeros, zeros))

in_vis_2 = np.concatenate((vis2, zeros_mnist, vis1, zeros_mnist))
labels_in_vis_2 = np.concatenate((vis2_lab, zeros, vis1_lab, zeros))

in_aud_1 = np.concatenate((zeros_coch, zeros_coch, aud2, aud1))
labels_in_aud_1 = np.concatenate((zeros, zeros, aud2_lab, aud1_lab))

in_aud_2 = np.concatenate((zeros_coch, aud1, zeros_coch, aud2))
labels_in_aud_2 = np.concatenate((zeros, aud1_lab, zeros, aud2_lab))

#Repeat with test data
vis1_test = test_mnist[:5000]
vis2_test = test_mnist[5000:]
vis1_lab_test = test_labels_mnist[:5000]
vis2_lab_test = test_labels_mnist[5000:]

aud1_test = test_cochleagrams[:5000]
aud2_test = test_cochleagrams[5000:]
aud1_lab_test = test_labels_cochleagrams[:5000]
aud2_lab_test = test_labels_cochleagrams[5000:]

zeros_test = np.zeros((5000,), dtype=int)
zeros_mnist_test = np.repeat([empty_mnist], 5000, axis=0)
zeros_coch_test = np.repeat([empty_coch], 5000, axis =0)

#create 4 input streams
## vis1  vis2  0    0 
## vis2   0    0    aud1
## 0     vis1 aud2  0 
## 0      0   aud1  aud2

in_vis_1_test = np.concatenate((vis1_test, vis2_test, zeros_mnist_test, zeros_mnist_test))
labels_in_vis_1_test = np.concatenate((vis1_lab_test, vis2_lab_test, zeros_test, zeros_test))

in_vis_2_test = np.concatenate((vis2_test, zeros_mnist_test, vis1_test, zeros_mnist_test))
labels_in_vis_2_test = np.concatenate((vis2_lab_test, zeros_test, vis1_lab_test, zeros_test))

in_aud_1_test = np.concatenate((zeros_coch_test, zeros_coch_test, aud2_test, aud1_test))
labels_in_aud_1_test = np.concatenate((zeros_test, zeros_test, aud2_lab_test, aud1_lab_test))

in_aud_2_test = np.concatenate((zeros_coch_test, aud1_test, zeros_coch_test, aud2_test))
labels_in_aud_2_test = np.concatenate((zeros_test, aud1_lab_test, zeros_test, aud2_lab_test))




#Input Visual 1
input_visual_1 = keras.layers.Input(shape=(28,28))
flatten_vis_1 = keras.layers.Flatten(input_shape=(28,28))(input_visual_1)
l1_vis_1 = keras.layers.Dense(32, activation='relu')(flatten_vis_1)

#Input Visual 2
input_visual_2 = keras.layers.Input(shape=(28,28))
flatten_vis_2 = keras.layers.Flatten(input_shape=(28,28))(input_visual_2)
l1_vis_2 = keras.layers.Dense(32, activation='relu')(flatten_vis_2)

#Input auditory 1
input_auditory_1 = keras.layers.Input(shape=(15,53))
flatten_aud_1 = keras.layers.Flatten(input_shape=(15,53))(input_auditory_1)
l1_aud_1 = keras.layers.Dense(32, activation='relu')(flatten_aud_1)

#Input auditory 2
input_auditory_2 = keras.layers.Input(shape=(15,53))
flatten_aud_2 = keras.layers.Flatten(input_shape=(15,53))(input_auditory_2)
l1_aud_2 = keras.layers.Dense(32, activation='relu')(flatten_aud_2)

#Concatenated
concatenated = keras.layers.Concatenate()([l1_vis_1, l1_vis_2, l1_aud_1, l1_aud_2])
l2 = keras.layers.Dense(64, activation = 'relu')(concatenated)
l3 = keras.layers.Dense(32, activation = 'relu')(l2)
out= keras.layers.Dense(19)(l3)

#Addition ....
train_labs_addition_visual_cochleagram = labels_in_vis_1 + labels_in_vis_2 + labels_in_aud_1 + labels_in_aud_2
print(train_labs_addition_visual_cochleagram)
print(max(train_labs_addition_visual_cochleagram))
print(min(train_labs_addition_visual_cochleagram))

test_labs_addition_visual_cochleagram = labels_in_vis_1_test + labels_in_vis_2_test + labels_in_aud_1_test + labels_in_aud_2_test
print(test_labs_addition_visual_cochleagram)
print(max(test_labs_addition_visual_cochleagram))
print(min(test_labs_addition_visual_cochleagram))

#Addition model
model_addition = keras.models.Model(inputs=[input_visual_1, input_visual_2, input_auditory_1, input_auditory_2], outputs = out)
model_addition.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
params_model_addition = model_addition.fit([in_vis_1, in_vis_2, in_aud_1, in_aud_2], train_labs_addition_visual_cochleagram, epochs = 5, shuffle=True)

#Evaluate model addition
test_loss, test_acc = model_addition.evaluate([in_vis_1_test, in_vis_2_test, in_aud_1_test, in_aud_2_test],  test_labs_addition_visual_cochleagram, verbose=2)
print('test acc:', test_acc)

all_loss_task = params_model_addition.history['loss']
all_acc_task = params_model_addition.history['accuracy']
plt.plot(all_loss_task, label='loss')
plt.plot(all_acc_task, label='accuracy')
plt.legend()
plt.show()

train_labs_bigger = []
train_labs_bigger[:30000] = labels_in_vis_1[:30000] > labels_in_vis_2[:30000]
train_labs_bigger[30000:60000] = labels_in_vis_1[30000:60000] > labels_in_aud_2[30000:60000]
train_labs_bigger[60000:90000] = labels_in_aud_1[60000:90000] > labels_in_vis_2[60000:90000]
train_labs_bigger[90000:] = labels_in_aud_1[90000:] > labels_in_aud_2[90000:]
train_labs_bigger = np.asarray(train_labs_bigger)
#print(train_labs_bigger)

#test_labs_addition_visual_cochleagram = 
test_labs_bigger = []
test_labs_bigger[:5000] = labels_in_vis_1_test[:5000] > labels_in_vis_2_test[:5000]
test_labs_bigger[5000:10000] = labels_in_vis_1_test[5000:10000] > labels_in_aud_2_test[5000:10000]
test_labs_bigger[10000:15000] = labels_in_aud_1_test[10000:15000] > labels_in_vis_2_test[10000:15000]
test_labs_bigger[15000:] = labels_in_aud_1_test[15000:] > labels_in_aud_2_test[15000:]
test_labs_bigger = np.asarray(test_labs_bigger)

#Bigger than
model_bigger = keras.models.Model(inputs=[input_visual_1, input_visual_2, input_auditory_1, input_auditory_2], outputs = out)
model_bigger.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
params_model_bigger = model_bigger.fit([in_vis_1, in_vis_2, in_aud_1, in_aud_2], train_labs_bigger, epochs = 2, shuffle=True)

#Evaluate model bigger than
test_loss, test_acc = model_bigger.evaluate([in_vis_1_test, in_vis_2_test, in_aud_1_test, in_aud_2_test],  test_labs_bigger, verbose=2)
print('test acc:', test_acc)

all_loss_task = params_model_bigger.history['loss']
all_acc_task = params_model_bigger.history['acc']
plt.plot(all_loss_task, label='loss')
plt.plot(all_acc_task, label='accuracy')
plt.legend()
plt.show()

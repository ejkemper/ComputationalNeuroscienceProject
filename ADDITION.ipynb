{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "#Import preprocessed data\n",
    "#Import packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.io import loadmat, savemat\n",
    "from os import listdir\n",
    "from os.path import dirname, join as join\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "#import visual representation - mnist\n",
    "mnist =  keras.datasets.mnist\n",
    "(train_imgs_mnist, train_labs_mnist), (test_imgs_mnist, test_labs_mnist) = mnist.load_data()\n",
    "\n",
    "train_imgs_mnist = train_imgs_mnist/255.0\n",
    "test_imgs_mnist = test_imgs_mnist/255.0\n",
    "\n",
    "#import cochleagrams \n",
    "def load_files(dir): #coch=15,53,\n",
    "    filenames = listdir(dir)\n",
    "    files = np.zeros((len(filenames), 15, 53))\n",
    "    labels = []\n",
    "    for i in range(len(filenames)):\n",
    "        file = np.load(join(dir, filenames[i]))\n",
    "        files[i] = file\n",
    "        label = int(filenames[i][0]) #to get the 6th character in the string, which gives the digit pronounced\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "    return files, labels\n",
    "\n",
    "dir = \"C:/Users/s141554/Documents/Systems Biology/Computational neuroscience/cochleagrams-preprocessed\"\n",
    "data_coch, labels_coch = load_files(dir)\n",
    "train_data_ch, test_data_ch, train_label_ch, test_label_ch = train_test_split(data_coch, labels_coch, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input - make sure for every number the amount of visual and auditory presentations are the same \n",
    "def match_data(data_mnist, labels_mnist, data_coch, labels_coch):\n",
    "    data_ch = []\n",
    "    data_m = []\n",
    "    labels = []\n",
    "    for x in range(10):\n",
    "        nr_mnist = len(labels_mnist[labels_mnist == x])\n",
    "        nr_coch = len(labels_coch[labels_coch == x])\n",
    "        reps = np.ceil(nr_mnist/nr_coch)\n",
    "\n",
    "        rep_data_ch = np.repeat(data_coch[labels_coch == x], reps, axis=0)\n",
    "        rep_data_ch = rep_data_ch[:nr_mnist]\n",
    "        data_ch.append(rep_data_ch)\n",
    "\n",
    "        data_m.append(data_mnist[labels_mnist == x])\n",
    "        labels.append(labels_mnist[labels_mnist == x])\n",
    "\n",
    "    data_ch = np.concatenate(data_ch, axis=0)\n",
    "    data_m = np.concatenate(data_m, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return data_ch, data_m, labels\n",
    "#output (created with match_data)\n",
    "train_data_cochleagrams, train_data_visual, train_label = match_data(train_imgs_mnist, train_labs_mnist, train_data_ch, train_label_ch)\n",
    "test_data_cochleagrams, test_data_visual, test_label = match_data(test_imgs_mnist, test_labs_mnist, test_data_ch, test_label_ch)\n",
    "\n",
    "#create random index order\n",
    "\n",
    "index_visual=np.arange(60000)\n",
    "np.random.shuffle(index_visual)\n",
    "\n",
    "index_auditory=np.arange(60000)\n",
    "np.random.shuffle(index_auditory)\n",
    "\n",
    "\n",
    "index_visual_test = np.arange(10000)\n",
    "np.random.shuffle(index_visual_test)\n",
    "index_auditory_test = np.arange(10000)\n",
    "np.random.shuffle(index_auditory_test)\n",
    "#randomize the order of the cochleagrams and mnist while keeping them coupled to the right label\n",
    "\n",
    "train_cochleagrams = train_data_cochleagrams[index_auditory]\n",
    "train_mnist= train_data_visual[index_visual]\n",
    "train_labels_cochleagrams = train_label[index_auditory]\n",
    "train_labels_mnist = train_label[index_visual]\n",
    "\n",
    "\n",
    "test_cochleagrams = test_data_cochleagrams[index_auditory_test]\n",
    "test_mnist = test_data_visual[index_visual_test]\n",
    "test_labels_cochleagrams = test_label[index_auditory_test]\n",
    "test_labels_mnist = test_label[index_visual_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare data for the task classifiers \n",
    "#Break training data into blocks\n",
    "vis1 = train_mnist[:30000]\n",
    "vis2 = train_mnist[30000:]\n",
    "vis1_lab = train_labels_mnist[:30000] \n",
    "vis2_lab = train_labels_mnist[30000:]\n",
    "\n",
    "aud1 = train_cochleagrams[:30000]\n",
    "aud2 = train_cochleagrams[30000:]\n",
    "aud1_lab = train_labels_cochleagrams[:30000]\n",
    "aud2_lab = train_labels_cochleagrams[30000:]\n",
    "\n",
    "zeros = np.zeros((30000,), dtype=int)\n",
    "empty_mnist = np.zeros((28,28),dtype=int)\n",
    "zeros_mnist = np.repeat([empty_mnist], 30000, axis=0)\n",
    "empty_coch = np.zeros((15,53),dtype=int)\n",
    "zeros_coch = np.repeat([empty_coch], 30000, axis =0)\n",
    "\n",
    "#create 4 input streams\n",
    "## vis1  vis2  0    0 \n",
    "## vis2   0    0    aud1\n",
    "## 0     vis1 aud2  0 \n",
    "## 0      0   aud1  aud2\n",
    "in_vis_1 = np.concatenate((vis1, vis2, zeros_mnist, zeros_mnist))\n",
    "labels_in_vis_1 = np.concatenate((vis1_lab, vis2_lab, zeros, zeros))\n",
    "\n",
    "in_vis_2 = np.concatenate((vis2, zeros_mnist, vis1, zeros_mnist))\n",
    "labels_in_vis_2 = np.concatenate((vis2_lab, zeros, vis1_lab, zeros))\n",
    "\n",
    "in_aud_1 = np.concatenate((zeros_coch, zeros_coch, aud2, aud1))\n",
    "labels_in_aud_1 = np.concatenate((zeros, zeros, aud2_lab, aud1_lab))\n",
    "\n",
    "in_aud_2 = np.concatenate((zeros_coch, aud1, zeros_coch, aud2))\n",
    "labels_in_aud_2 = np.concatenate((zeros, aud1_lab, zeros, aud2_lab))\n",
    "\n",
    "training_input = [in_vis_1, in_vis_2, in_aud_1, ]\n",
    "\n",
    "#Repeat with test data\n",
    "vis1_test = test_mnist[:5000]\n",
    "vis2_test = test_mnist[5000:]\n",
    "vis1_lab_test = test_labels_mnist[:5000]\n",
    "vis2_lab_test = test_labels_mnist[5000:]\n",
    "\n",
    "aud1_test = test_cochleagrams[:5000]\n",
    "aud2_test = test_cochleagrams[5000:]\n",
    "aud1_lab_test = test_labels_cochleagrams[:5000]\n",
    "aud2_lab_test = test_labels_cochleagrams[5000:]\n",
    "\n",
    "zeros_test = np.zeros((5000,), dtype=int)\n",
    "zeros_mnist_test = np.repeat([empty_mnist], 5000, axis=0)\n",
    "zeros_coch_test = np.repeat([empty_coch], 5000, axis =0)\n",
    "\n",
    "#create 4 input streams\n",
    "## vis1  vis2  0    0 \n",
    "## vis2   0    0    aud1\n",
    "## 0     vis1 aud2  0 \n",
    "## 0      0   aud1  aud2\n",
    "\n",
    "in_vis_1_test = np.concatenate((vis1_test, vis2_test, zeros_mnist_test, zeros_mnist_test))\n",
    "labels_in_vis_1_test = np.concatenate((vis1_lab_test, vis2_lab_test, zeros_test, zeros_test))\n",
    "\n",
    "in_vis_2_test = np.concatenate((vis2_test, zeros_mnist_test, vis1_test, zeros_mnist_test))\n",
    "labels_in_vis_2_test = np.concatenate((vis2_lab_test, zeros_test, vis1_lab_test, zeros_test))\n",
    "\n",
    "in_aud_1_test = np.concatenate((zeros_coch_test, zeros_coch_test, aud2_test, aud1_test))\n",
    "labels_in_aud_1_test = np.concatenate((zeros_test, zeros_test, aud2_lab_test, aud1_lab_test))\n",
    "\n",
    "in_aud_2_test = np.concatenate((zeros_coch_test, aud1_test, zeros_coch_test, aud2_test))\n",
    "labels_in_aud_2_test = np.concatenate((zeros_test, aud1_lab_test, zeros_test, aud2_lab_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#OLD VERSION \n",
    "#Input Visual 1\n",
    "input_visual_1 = keras.layers.Input(shape=(28,28))\n",
    "flatten_vis_1 = keras.layers.Flatten(input_shape=(28,28))(input_visual_1)\n",
    "l1_vis_1 = keras.layers.Dense(32, activation='relu')(flatten_vis_1)\n",
    "\n",
    "#Input Visual 2\n",
    "input_visual_2 = keras.layers.Input(shape=(28,28))\n",
    "flatten_vis_2 = keras.layers.Flatten(input_shape=(28,28))(input_visual_2)\n",
    "l1_vis_2 = keras.layers.Dense(32, activation='relu')(flatten_vis_2)\n",
    "\n",
    "#Input auditory 1\n",
    "input_auditory_1 = keras.layers.Input(shape=(15,53))\n",
    "flatten_aud_1 = keras.layers.Flatten(input_shape=(15,53))(input_auditory_1)\n",
    "l1_aud_1 = keras.layers.Dense(32, activation='relu')(flatten_aud_1)\n",
    "\n",
    "#Input auditory 2\n",
    "input_auditory_2 = keras.layers.Input(shape=(15,53))\n",
    "flatten_aud_2 = keras.layers.Flatten(input_shape=(15,53))(input_auditory_2)\n",
    "l1_aud_2 = keras.layers.Dense(32, activation='relu')(flatten_aud_2)\n",
    "\n",
    "#Concatenated\n",
    "concatenated = keras.layers.Concatenate()([l1_vis_1, l1_vis_2, l1_aud_1, l1_aud_2])\n",
    "l2 = keras.layers.Dense(64, activation = 'relu')(concatenated)\n",
    "l3 = keras.layers.Dense(32, activation = 'relu')(l2)\n",
    "out= keras.layers.Dense(19)(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import Model\n",
    "class Sequentialmodel(Model):\n",
    "    def __init__(self):\n",
    "        super(Sequentialmodel, self).__init__()\n",
    "        #Input Visual  sequential\n",
    "        self.input_mnist1 = tf.keras.Sequential([\n",
    "            keras.layers.Flatten(input_shape=(28,28)),\n",
    "            keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "        \n",
    "        #Input Visual  sequential\n",
    "        self.input_mnist2 = tf.keras.Sequential([\n",
    "            keras.layers.Flatten(input_shape=(28,28)),\n",
    "            keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "\n",
    "\n",
    "        #Input auditory sequential\n",
    "        self.input_coch1 = tf.keras.Sequential([\n",
    "            keras.layers.Flatten(input_shape=(15,53)),\n",
    "            keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "        \n",
    "         #Input auditory sequential\n",
    "        self.input_coch2 = tf.keras.Sequential([\n",
    "            keras.layers.Flatten(input_shape=(15,53)),\n",
    "            keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "\n",
    "        #Concatenated sequential \n",
    "        self.concatenated = tf.keras.Sequential([\n",
    "        keras.layers.Concatenate(axis=-1),\n",
    "        keras.layers.Dense(64, activation = 'relu'),\n",
    "        keras.layers.Dense(32, activation = 'relu')\n",
    "        ])\n",
    "        \n",
    "        #output sequential\n",
    "        self.out= tf.keras.Sequential([keras.layers.Dense(19)])\n",
    "        \n",
    "\n",
    "    \n",
    "    def encode(self, im1, im2, ch1, ch2):\n",
    "        encoded_im1 = self.input_mnist1(im1) \n",
    "        encoded_im2 = self.input_mnist2(im2)\n",
    "        encoded_ch1 = self.input_coch1(ch1)\n",
    "        encoded_ch2 = self.input_coch2(ch2)\n",
    "        combined_encoding = self.concatenated([encoded_im1, encoded_im2, encoded_ch1, encoded_ch2])\n",
    "        return combined_encoding\n",
    "    \n",
    "    def output(self, encoded):\n",
    "        out = self.out(encoded)\n",
    "        return out\n",
    "    \n",
    "    def call(self, input):\n",
    "        im1, im2, ch1, ch2 = input\n",
    "        encoding = self.encode(im1, im2, ch1, ch2)\n",
    "        out = self.output(encoding)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  3  6 ...  9 13 10]\n",
      "18\n",
      "0\n",
      "[ 6  9  6 ... 10  7  5]\n",
      "18\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Addition ....\n",
    "train_labs_addition_visual_cochleagram = labels_in_vis_1 + labels_in_vis_2 + labels_in_aud_1 + labels_in_aud_2\n",
    "print(train_labs_addition_visual_cochleagram)\n",
    "print(max(train_labs_addition_visual_cochleagram))\n",
    "print(min(train_labs_addition_visual_cochleagram))\n",
    "\n",
    "test_labs_addition_visual_cochleagram = labels_in_vis_1_test + labels_in_vis_2_test + labels_in_aud_1_test + labels_in_aud_2_test\n",
    "print(test_labs_addition_visual_cochleagram)\n",
    "print(max(test_labs_addition_visual_cochleagram))\n",
    "print(min(test_labs_addition_visual_cochleagram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s141554\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Sequentialmodel.call of <__main__.Sequentialmodel object at 0x000001C47FDF3C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sequentialmodel.call of <__main__.Sequentialmodel object at 0x000001C47FDF3C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Sequentialmodel.call of <__main__.Sequentialmodel object at 0x000001C47FDF3C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sequentialmodel.call of <__main__.Sequentialmodel object at 0x000001C47FDF3C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 1/5\n",
      "120000/120000 [==============================] - 10s 79us/sample - loss: 1.9700 - acc: 0.2985\n",
      "Epoch 2/5\n",
      "120000/120000 [==============================] - 9s 73us/sample - loss: 0.9743 - acc: 0.6720\n",
      "Epoch 3/5\n",
      "120000/120000 [==============================] - 9s 72us/sample - loss: 0.5967 - acc: 0.8172\n",
      "Epoch 4/5\n",
      "120000/120000 [==============================] - 8s 71us/sample - loss: 0.4105 - acc: 0.8857\n",
      "Epoch 5/5\n",
      "120000/120000 [==============================] - 9s 72us/sample - loss: 0.3065 - acc: 0.9184\n",
      "20000/20000 - 1s - loss: 1.7466 - acc: 0.6335\n",
      "test acc: 0.6335\n"
     ]
    }
   ],
   "source": [
    "#Addition model sequential\n",
    "model_add = Sequentialmodel()\n",
    "model_add.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model_add.fit([in_vis_1, in_vis_2, in_aud_1, in_aud_2], train_labs_addition_visual_cochleagram, epochs = 5, shuffle=True)\n",
    "\n",
    "#Evaluate model addition\n",
    "test_loss, test_acc = model_add.evaluate([in_vis_1_test, in_vis_2_test, in_aud_1_test, in_aud_2_test],  test_labs_addition_visual_cochleagram, verbose=2)\n",
    "print('test acc:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "120000/120000 [==============================] - 26s 219us/sample - loss: 1.9733 - acc: 0.3046\n",
      "Epoch 2/5\n",
      "120000/120000 [==============================] - 20s 167us/sample - loss: 0.9977 - acc: 0.6648\n",
      "Epoch 3/5\n",
      "120000/120000 [==============================] - 18s 153us/sample - loss: 0.6355 - acc: 0.7985\n",
      "Epoch 4/5\n",
      "120000/120000 [==============================] - 16s 136us/sample - loss: 0.4561 - acc: 0.8665\n",
      "Epoch 5/5\n",
      "120000/120000 [==============================] - 14s 120us/sample - loss: 0.3467 - acc: 0.9042\n",
      "20000/20000 - 1s - loss: 1.6716 - acc: 0.6263\n",
      "test acc: 0.6263\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1793886f8383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mall_loss_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_model_addition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mall_acc_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_model_addition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_loss_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_acc_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "#OLD VERSION \n",
    "#Addition model\n",
    "model_addition = keras.models.Model(inputs=[input_visual_1, input_visual_2, input_auditory_1, input_auditory_2], outputs = out)\n",
    "model_addition.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "params_model_addition = model_addition.fit([in_vis_1, in_vis_2, in_aud_1, in_aud_2], train_labs_addition_visual_cochleagram, epochs = 5, shuffle=True)\n",
    "\n",
    "#Evaluate model addition\n",
    "test_loss, test_acc = model_addition.evaluate([in_vis_1_test, in_vis_2_test, in_aud_1_test, in_aud_2_test],  test_labs_addition_visual_cochleagram, verbose=2)\n",
    "print('test acc:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(Model):\n",
    "    def __init__(self, model):\n",
    "        super(Classifier, self).__init__()\n",
    "        # because we have a binary classifier, sigmoid is more suitable\n",
    "        activation = 'sigmoid'\n",
    "        # self.model: keras.Model = tf.keras.models.load_model(dir, custom_objects={\"seperate_loss\": seperate_loss})\n",
    "        self.model = model\n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            keras.layers.Dense(1, activation=activation)  # latent space\n",
    "        ])\n",
    "\n",
    "    def call(self, input):\n",
    "        im1, im2, ch1, ch2 = input\n",
    "        latent_space = self.model.encode(im1, im2, ch1, ch2)\n",
    "        classified = self.classifier(latent_space)\n",
    "        return classified\n",
    "\n",
    "\n",
    "def make_dataset(data_im, data_ch):\n",
    "    empty_mnist = np.zeros((28,28),dtype=int)\n",
    "    empty_coch = np.zeros((15,53),dtype=int)\n",
    "    data_im_zeros = np.repeat([empty_mnist], int(len(data_im)/2), axis=0)\n",
    "    data_ch_zeros = np.repeat([empty_coch], int(len(data_im)/2), axis=0)\n",
    "    \n",
    "    data_im1 = data_im[:int(len(data_im)/2)]\n",
    "    data_im2 = data_im[int(len(data_im)/2):]\n",
    "    data_ch1 = data_ch[:int(len(data_im)/2)]\n",
    "    data_ch2 = data_ch[int(len(data_im)/2):]\n",
    "    \n",
    "\n",
    "    data_1 = np.concatenate([data_im1, data_im2, data_im_zeros, data_im_zeros], axis=0)\n",
    "    data_2 = np.concatenate([data_im2, data_im1, data_im_zeros, data_im_zeros], axis=0)\n",
    "    data_3 = np.concatenate([data_ch_zeros, data_ch_zeros, data_ch1, data_ch2], axis=0)\n",
    "    data_4 = np.concatenate([data_ch_zeros, data_ch_zeros, data_ch2, data_ch1], axis=0)\n",
    "\n",
    "    #if input is an image, output 1, if input is a cochleagram output 0\n",
    "    im_lab = np.ones(len(data_im))\n",
    "    ch_lab = np.zeros(len(data_ch))\n",
    "    labels = np.concatenate([im_lab, ch_lab])\n",
    "\n",
    "    \n",
    "    return data_1, data_2, data_3, data_4, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "WARNING:tensorflow:Entity <bound method Classifier.call of <__main__.Classifier object at 0x000001C42F0C79C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Classifier.call of <__main__.Classifier object at 0x000001C42F0C79C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Classifier.call of <__main__.Classifier object at 0x000001C42F0C79C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Classifier.call of <__main__.Classifier object at 0x000001C42F0C79C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-12-32885d1d9cb1>:17 call\n        latent_space = self.model.encode(self, im1, im2, ch1, ch2)\n\n    TypeError: encode() takes 5 positional arguments but 6 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5ce1e73b8b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                        metrics=['accuracy'])\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_ls_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_ls_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_ls_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_ls_4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_latent_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_testls_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_testls_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_testls_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_testls_4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels_latent_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2556\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2558\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2559\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2560\u001b[0m       \u001b[0my_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m   2774\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2775\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2776\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2777\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m         \u001b[1;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-12-32885d1d9cb1>:17 call\n        latent_space = self.model.encode(self, im1, im2, ch1, ch2)\n\n    TypeError: encode() takes 5 positional arguments but 6 were given\n"
     ]
    }
   ],
   "source": [
    "data_ls_1, data_ls_2, data_ls_3, data_ls_4, train_labels_latent_space = make_dataset(train_mnist, train_cochleagrams)\n",
    "data_testls_1, data_testls_2, data_testls_3, data_testls_4, test_labels_latent_space = make_dataset(test_mnist, test_cochleagrams)\n",
    "print(train_labels_latent_space)\n",
    "\n",
    "classifier = Classifier(model_add)\n",
    "classifier.compile(optimizer='adam',\n",
    "                       loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       # since input are values, no probabilities\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "classifier.fit([data_ls_1, data_ls_2, data_ls_3, data_ls_4], train_labels_latent_space, epochs=5, shuffle=True)\n",
    "test_loss, test_acc = classifier.evaluate([data_testls_1, data_testls_2, data_testls_3, data_testls_4], test_labels_latent_space, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
